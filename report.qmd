---
title: "CMSC 436 — Project 1"
author: "Your Team Name"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Team & Certification

- Team member 1: _Isabella Darko / Normalized and plotted data set B_
- Team member 2: _Hudson Finocchio / Normalized and plotted data set C_
- Team member 3: _Name / contribution_
- Team member 4: _Jacob Alexander / Created truth table and logic function_

(Attach the signed certification page from the assignment PDF.)

# Part 1 — Dataset Exploration and Linear Separation

## Dataset A

## Dataset B

### Q1 — Normalize and visualize Dataset B

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load the dataset into a DataFrame
dfB = pd.read_csv("data/groupB.txt", header=None, names=["price","weight","type"])

# 2. Normalize price and weight (min-max)
dfB["price"] = (dfB["price"] - dfB["price"].min()) / (dfB["price"].max() - dfB["price"].min())
dfB["weight"] = (dfB["weight"] - dfB["weight"].min()) / (dfB["weight"].max() - dfB["weight"].min())

# 3. Show first 5 rows as an HTML table
dfB.head()
```

```{python}
# 4. Scatter plot of normalized data

# Split the dataset into two groups based on the "type" column
# type = 0 → small cars, type = 1 → big cars
small = dfB[dfB["type"]==0]
big   = dfB[dfB["type"]==1]

# Start a new figure
plt.figure()

# Plot the small cars (purple dots)
# x-axis = normalized price, y-axis = normalized weight
# alpha=0.6 makes the dots slightly transparent so overlaps are visible
plt.scatter(small["price"], small["weight"], alpha=0.6, label="Small (0)", color="purple")

# Plot the big cars (pink dots)
plt.scatter(big["price"],   big["weight"],   alpha=0.6, label="Big (1)", color="pink")

# Add axis labels
plt.xlabel("Price (normalized)")
plt.ylabel("Weight (normalized)")

# Add a plot title
plt.title("Dataset B — Two Categories (Normalized)")

# Show the legend so we know which color = which type
plt.legend()

# Adjust the layout so labels/titles don't overlap
plt.tight_layout()

# Save the figure to a PNG file (in the figs/ folder)
plt.savefig("figs/B_scatter.png", dpi=150)

# Display the figure in the Quarto-rendered HTML report
plt.show()
```

### Q2 — Drawing my line on top of the scatter

```{python}

import numpy as np
import matplotlib.pyplot as plt

m, b = -1.0, 1.0   # slope and intercept for the line

# split the data into small and big again
small = dfB[dfB["type"] == 0]
big   = dfB[dfB["type"] == 1]

# x values from 0 to 1 since everything is normalized
x_vals = np.linspace(0, 1, 500)
y_vals = m * x_vals + b

plt.figure()

# scatter plot of both groups (purple circles vs pink squares)
plt.scatter(small["price"], small["weight"], alpha=0.6, label="Small (0)", color="purple", marker="o")
plt.scatter(big["price"],   big["weight"],   alpha=0.6, label="Big (1)",   color="pink",   marker="s")

# plot the line across the graph
plt.plot(x_vals, y_vals, color="black", linewidth=2, label=f"y = {m}x + {b}")

# keep the axes locked to the normalized range
plt.xlim(0, 1)
plt.ylim(0, 1)

plt.xlabel("Price (normalized)")
plt.ylabel("Weight (normalized)")
plt.title("Dataset B — Slope Seperator")
plt.legend()
plt.tight_layout()
plt.savefig("figs/B_separator.png", dpi=150)
plt.show()
```

### Q3 — Turning line into the neuron inequality

```{python}
# my line is: y = m*x + b
m, b = -1.0, 1.0

# flip it into neuron form: w1*x1 + w2*x2 >= theta
w1 = -m     # weight for price
w2 = 1.0    # weight for weight
theta = b   # threshold

print("my weights and threshold:")
print("w1 =", w1)
print("w2 =", w2)
print("theta =", theta)

# now use these to classify points in dfB
net = w1*dfB["price"] + w2*dfB["weight"]
pred = (net >= theta).astype(int)

# just show a few rows to check
dfB.assign(pred=pred)[["price","weight","type","pred"]].head()

```

### Q4 - Confusion matrix (TP, TN, FP, FN)

```{python}
# true labels and my predictions from Q3
y_true = dfB["type"].astype(int)
y_pred = pred.astype(int)

TP = int(((y_true == 1) & (y_pred == 1)).sum())
TN = int(((y_true == 0) & (y_pred == 0)).sum())
FP = int(((y_true == 0) & (y_pred == 1)).sum())
FN = int(((y_true == 1) & (y_pred == 0)).sum())

print("confusion matrix counts:")
print("TP:", TP, "  FN:", FN)
print("FP:", FP, "  TN:", TN)

# table
import pandas as pd
pd.DataFrame(
    [[TP, FN],
     [FP, TN]],
    index=["Actual 1 (Big)", "Actual 0 (Small)"],
    columns=["Pred 1 (Big)", "Pred 0 (Small)"]
)
```

### Q5 Accuracy and Rates

```{python}
# I already have TP, TN, FP, FN from Q4
N = TP + TN + FP + FN

accuracy   = (TP + TN) / N if N else float("nan")
error_rate = 1 - accuracy if N else float("nan")

TPR = TP / (TP + FN) if (TP + FN) else float("nan")   # recall for big (class 1)
TNR = TN / (TN + FP) if (TN + FP) else float("nan")   # specificity for small (class 0)
FPR = FP / (FP + TN) if (FP + TN) else float("nan")   # false alarms: small → big
FNR = FN / (FN + TP) if (FN + TP) else float("nan")   # misses: big → small

print("metrics:")
print("accuracy:", round(accuracy, 4))
print("error_rate:", round(error_rate, 4))
print("TPR:", round(TPR, 4))
print("TNR:", round(TNR, 4))
print("FPR:", round(FPR, 4))
print("FNR:", round(FNR, 4))

# table for the report
import pandas as pd
pd.DataFrame([{
    "Accuracy": accuracy,
    "Error": error_rate,
    "TPR": TPR,
    "TNR": TNR,
    "FPR": FPR,
    "FNR": FNR
}]).round(4)
```

## Dataset C

### Q1 — Normalize and visualize Dataset C
```{python}
import numpy as np
import matplotlib.patches as mpatches

# input data: price (USD), weight (lbs), and size (small (0) or big (1))
group_c_df = pd.read_csv("data/groupC.txt", header=None, names=["price","weight","size"])

# normalize columns in dataframe
def normalize(df: pd.DataFrame, columns: list):

    df_norm = df.copy()

    for column in columns:
        min_value = df_norm[column].min()
        max_value = df_norm[column].max()
        df_norm[column] = (df_norm[column] - min_value) / (max_value - min_value)

    return df_norm

processed_df = normalize(group_c_df, columns=['price', 'weight'])

processed_df.head(5)
```
```{python}

# prepare axes and viz for points
x_axis = processed_df['weight'].values
y_axis = processed_df['price'].values
colors = processed_df['size'].values

# plot everything
plt.figure(figsize=(6, 6))
plt.scatter(x_axis, y_axis, c=colors, cmap="bwr", alpha=0.6, s=80, edgecolors="k")
plt.xlabel("weight (lbs)")
plt.ylabel("price (USD)")
plt.title("price and weight of cars of varying size")

plt.grid(True)
plt.show()
```
### Q2 — Drawing my line on top of the scatter
```{python}
# estimated function
def f(x):
    return -.79*x + .88

# get values for function

x = np.linspace(0, 1, 30)
y = f(x)

# prepare axes and viz for points
x_axis = processed_df['weight'].values
y_axis = processed_df['price'].values
colors = processed_df['size'].values

# plot everything again
plt.figure(figsize=(6, 6))
plt.scatter(x_axis, y_axis, c=colors, cmap="bwr", alpha=0.6, s=80, edgecolors="k")
plt.xlabel("weight (lbs)")
plt.ylabel("price (USD)")
plt.title("price and weight of cars of varying size")

plt.plot(x, y, color='green', linestyle='--', linewidth=2, label="y =  -0.79x + 0.88")
small_patch = mpatches.Patch(color='blue', label='Small (0)')
big_patch = mpatches.Patch(color='red', label='Big (1)')

plt.legend(handles=[small_patch, big_patch],
           title="Car size",
           loc="upper right")

plt.grid(True)
plt.show()
```
### Q3 — Turning line into the neuron inequality

We start with the estimated linear function:

$$
\text{price} = -0.79 \cdot \text{weight} + 0.88
$$

Rearranging into linear form:

$$
y + 0.79x - 0.88 = 0
$$

which is equivalent to:

$$
1.0 \cdot \text{price} + 0.79 \cdot \text{weight} - 0.88 = 0
$$

The activation is the weighted sum:

$$
a(x, y) \le 0 \quad\Longrightarrow\quad 0.79\cdot\text{weight} + \text{price} \le 0.88
$$

### Q4 - Confusion matrix (TP, TN, FP, FN)
```{python}
from IPython.display import display

pred_labels = (0.79 * processed_df['weight'] + processed_df['price'] >= 0.88).astype(int)

# true labels
true_labels = processed_df['size'].astype(int)

# confusion matrix components
TP = ((pred_labels == 1) & (true_labels == 1)).sum()  # true positive - predicted big & actually big
TN = ((pred_labels == 0) & (true_labels == 0)).sum()  # true negative - predicted small & actually small
FP = ((pred_labels == 1) & (true_labels == 0)).sum()  # false positive - predicted big & actually small
FN = ((pred_labels == 0) & (true_labels == 1)).sum()  # false negative - predicted small & actually big

# display confusion matrix
confusion_matrix = pd.DataFrame(
    [[TP, FP],
     [FN, TN]],
    index=['Actual Big (1)', 'Actual Small (0)'],
    columns=['Predicted Big (1)', 'Predicted Small (0)']
)

print("Confusion Matrix:")
display(confusion_matrix)
```
### Q5 Accuracy and Rates
```{python}
# metrics / rates
accuracy = (TP + TN) / (TP + TN + FP + FN)
error_rate = 1 - accuracy
true_positive_rate = TP / (TP + FN) 
true_negative_rate = TN / (TN + FP)  
false_positive_rate = FP / (FP + TN)
false_negative_rate = FN / (FN + TP)

print(f"Accuracy: {accuracy:.3f}")
print(f"Error rate: {error_rate:.3f}")
print(f"True Positive Rate (TPR / Recall for big): {true_positive_rate:.3f}")
print(f"True Negative Rate (TNR / Specificity): {true_negative_rate:.3f}")
print(f"False Positive Rate (FPR): {false_positive_rate:.3f}")
print(f"False Negative Rate (FNR): {false_negative_rate:.3f}")
```

### Q6 - Similarity of All Data Sets

# Comparison of Datasets and Metrics

## 1. Confusion and Accuracy

| Dataset | Separation | Accuracy | TPR (Big) | TNR (Small) | FPR  | FNR  |
|---------|------------|----------|------------|-------------|------|------|
| A       | Most      | 1     | 1      | 1        | 0  | 0  |
| B       | Moderate  | 0.987   | 0.991     | 0.984      | 0.016 | 0.009 |
| C       | Least     | 0.725      | 0.739        | 0.711         | 0.289 | 0.261 |

- **Dataset A**: Big and small cars are clearly clustered →  All predictions correct.  
- **Dataset B**: Classes start to overlap → some misclassifications.  
- **Dataset C**: Strong overlap → many false positives and false negatives, lower accuracy.

---

## 2. Why the datasets differ

- Datasets represent progressively more challenging classification problems:  
  - A: clearly separated clusters  
  - B: partially overlapping  
  - C: heavily overlapping  

---

## 3. Role of Normalization

- Features (price, weight) have different scales.  
- Normalizing to [0,1] ensures **both features contribute proportionally** to the linear separator.  
- Without normalization, the larger-scale feature would dominate the decision, reducing classifier performance.

---

# Part 2 — McCulloch–Pitts Neuron

**Neuron Output Rule**

$$
\text{Output} =
\begin{cases}
1, & \text{if } \text{net} \geq 0 \\
0, & \text{if } \text{net} < 0
\end{cases}
$$

---

**Truth Table**

| X | Y | Z | Net = \(-3X + 3Y + Z\) | Inequality        | Output |
|---|---|---|-------------------------|------------------|--------|
| 0 | 0 | 0 | \(-3(0) + 3(0) + 0 = 0\)  | \(0 \geq -1\)     | 1      |
| 0 | 0 | 1 | \(-3(0) + 3(0) + 1 = 1\)  | \(1 \geq -1\)     | 1      |
| 0 | 1 | 0 | \(-3(0) + 3(1) + 0 = 3\)  | \(3 \geq -1\)     | 1      |
| 0 | 1 | 1 | \(-3(0) + 3(1) + 1 = 4\)  | \(4 \geq -1\)     | 1      |
| 1 | 0 | 0 | \(-3(1) + 3(0) + 0 = -3\) | \(-3 < -1\)       | 0      |
| 1 | 0 | 1 | \(-3(1) + 3(0) + 1 = -2\) | \(-2 < -1\)       | 0      |
| 1 | 1 | 0 | \(-3(1) + 3(1) + 0 = 0\)  | \(0 \geq -1\)     | 1      |
| 1 | 1 | 1 | \(-3(1) + 3(1) + 1 = 1\)  | \(1 \geq -1\)     | 1      |

---

**Logic Function**

$$
y = X + Y \quad \text{(equivalently } y = \lnot X \lor Y \text{)}
$$

*Justification:* The neuron fires (\(y=1\)) whenever \(X=0\) or \(Y=1\), regardless of the value of \(Z\).

---

**Activation Range**

$$
\text{Range of net values: } (-2, \; 0]
$$

